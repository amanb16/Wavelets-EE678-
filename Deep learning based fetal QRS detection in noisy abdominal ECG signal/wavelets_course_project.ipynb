{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"Wavelets Course Project.ipynb\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Commented out IPython magic to ensure Python compatibility."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n", "print(tf.test.gpu_device_name())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os \n", "if 'COLAB_TPU_ADDR' not in os.environ:\n", "  print('Not connected to TPU')\n", "else:\n", "  print('Connected to TPU')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from google.colab import drive\n", "drive.mount('/mntDrive')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Some Installations\n<br>\n", "!pip install pyEDFlib<br>\n", " Some Modules\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings \n", "warnings.simplefilter('ignore')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import numpy.matlib as nmp\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import pywt \n", "import time\n", "import pyedflib"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Using **pyedflib** ( Data exloration of file r01.edf ) <br>\n", "Data taken from https://physionet.org/content/adfecgdb/1.0.0/ : Abdominal and Direct Fetal ECG Database<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["a = pyedflib.highlevel.read_edf('ecgca102.edf', ch_nrs=None, ch_names=None, digital=False, verbose=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Parameters:\t<br>\n", "edf_file : str<br>\n", "link to an edf file."]}, {"cell_type": "markdown", "metadata": {}, "source": ["ch_nrs : list of int, optional<br>\n", "The indices of the channels to read. The default is None."]}, {"cell_type": "markdown", "metadata": {}, "source": ["ch_names : list of str, optional<br>\n", "The names of channels to read. The default is None."]}, {"cell_type": "markdown", "metadata": {}, "source": ["digital : bool, optional<br>\n", "will return the signals as digital values (ADC). The default is False."]}, {"cell_type": "markdown", "metadata": {}, "source": ["verbose : bool, optional<br>\n", "Print progress bar while loading or not. The default is False."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Returns:\t<br>\n", "signals : np.ndarray or list<br>\n", "the signals of the chosen channels contained in the EDF."]}, {"cell_type": "markdown", "metadata": {}, "source": ["signal_headers : list<br>\n", "one signal header for each channel in the EDF."]}, {"cell_type": "markdown", "metadata": {}, "source": ["header : dict<br>\n", "the main header of the EDF file containing meta information."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a = pyedflib.highlevel.read_edf('r01.edf', verbose=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(len(a[2]['annotations']))\n", "true_labels = a[2]['annotations']\n", "print(true_labels)   # These are fetal ECG annotations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(a[1])    # Gives us the information about signal headers."]}, {"cell_type": "markdown", "metadata": {}, "source": ["len(a[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["length_to_plot = 2000\n", "# Direct Signal 1\n", "plt.figure(figsize=(5,3))\n", "plt.plot(a[0][0][:length_to_plot+1])\n", "plt.title('Direct Signal (From the Scalp of Fetus)')\n", "plt.xlabel('Time (in ms)')\n", "plt.ylabel('Amplitude (in uV)')\n", "plt.savefig('Direct_Signal.png', bbox_inches = 'tight')\n", "plt.show()\n", "plt.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Abdomen Signal 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(5,3))\n", "plt.plot(a[0][1][:length_to_plot+1])\n", "plt.title('Abdominal Signal 1')\n", "plt.xlabel('Time (in ms)')\n", "plt.ylabel('Amplitude (in uV)')\n", "plt.savefig('Abdominal_Signal_1.png', bbox_inches = 'tight')\n", "plt.show()\n", "plt.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Abdomen Signal 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(5,3))\n", "plt.plot(a[0][2][:length_to_plot+1])\n", "plt.title('Abdominal Signal 2')\n", "plt.xlabel('Time (in ms)')\n", "plt.ylabel('Amplitude (in uV)')\n", "plt.savefig('Abdominal_Signal_2.png', bbox_inches = 'tight')\n", "plt.show()\n", "plt.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Abdomen Signal 3"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(5,3))\n", "plt.plot(a[0][3][:length_to_plot+1])\n", "plt.title('Abdominal Signal 3')\n", "plt.xlabel('Time (in ms)')\n", "plt.ylabel('Amplitude (in uV)')\n", "plt.savefig('Abdominal_Signal_3.png', bbox_inches = 'tight')\n", "plt.show()\n", "plt.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Abdomen Signal 4"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(5,3))\n", "plt.plot(a[0][4][:length_to_plot+1])\n", "plt.title('Abdominal Signal 4')\n", "plt.xlabel('Time (in ms)')\n", "plt.ylabel('Amplitude (in uV)')\n", "plt.savefig('Abdominal_Signal_4.png', bbox_inches = 'tight')\n", "plt.show()\n", "plt.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Tasks\n<br>\n", "# ####################################################################################<br>\n", "# Task 1<br>\n", "# ####################################################################################<br>\n", "# we have a signal (one of the 5 signals)<br>\n", "#<br>\n", "# decide on window length (512 ms, 1024 ms) and stride (50 ms)<br>\n", "# <br>\n", "# intermediate task: create a few scalograms and have a look at them (to check if what we actually think is indeed happening)<br>\n", "# <br>\n", "# create and store scalograms <br>\n", "#<br>\n", "# repeat these steps for all 5 signals <br>\n", "#<br>\n", "# repeat these steps for all 5 signals with l = 1024 <br>\n", "# <br>\n", "# NOTE that we are doing the analysis only for stride = 50ms <br>\n", "# reducing the stride value further (e.g. 25ms etc) would be beneficial to increase the accuracy in obtaining the Fetal heart rate<br>\n", "#<br>\n", "# ####################################################################################<br>\n", "# Try running everything using the scalograms generated with local vmin and vmax<br>\n", "# This is absurd theoratically but let's see what the DL model gives us<br>\n", "# DL model might make sense (minute possibility) if it infers contrast & shapes more than colors ONLY<br>\n", "# ####################################################################################<br>\n", "#<br>\n", "# remove the shitty noise (imp even for the idea of stacked scalograms as an input to the DL)<br>\n", "# <br>\n", "# remove noise (using notch filter???) at 50 Hz (Nithin + Aman)<br>\n", "# repeat the entire analysis <br>\n", "# <br>\n", "# ####################################################################################<br>\n", "# ####################################################################################<br>\n", "# <br>\n", "# FINISHABLE BY TONIGHT:<br>\n", "# Use simple CNN (with both \"15 (or 12??) channel\" and \"3 channel + mode of ouput\" methods) and classify : FINISHABLE BY TONIGHT<br>\n", "# <br>\n", "# Will go perhaps in FUTURE WORK:<br>\n", "# Fine tune the VGG12, DenseNet etc (for both \"15 (or 12??) channel\" and \"3 channel + mode of ouput\" methods)<br>\n", "# NOTE: you'll need to add a layer in the beginning to be able to employ \"12 channel\" method<br>\n", "# <br>\n", "# NOTE: Do we even need to have 3 channels to represent coeff of CWT??? and then further take 4 such CWTs (i.e. 4 * 3 channels)??? Why not straigtaway give CWT coeff as one channel?? (rather than converting these CWT coeff to 3 channels etc) [Funda is that CNN is expecting images as inputs]<br>\n", "# ####################################################################################<br>\n", "# ####################################################################################<br>\n", " Analysis for Abdominal Signal 1, 2, 3, 4 and Direct Signal \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["z = 3 # for abdominal signal 1; just change this for other abdominal signals\n", "abdominal_signal = a[0][z]    # Abdominal signal 1\n", "# abdominal_signal = a[0][0]    # Direct Signal\n", "print(type(abdominal_signal))\n", "print(len(abdominal_signal))\n", "print(np.count_nonzero(np.isnan(abdominal_signal)))    # check for any missing values\n", "plt.plot(abdominal_signal[:3000])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Compute a required variable\n<br>\n", "l = 512    # consider window length l = 512, i.e. 512/1000 secs<br>\n", "s = 50    # consider stliding stride s = 50, i.e. 50/1000 secs <br>\n", "t = len(abdominal_signal)-1<br>\n", "# n_final_false = t-l+1<br>\n", "# print(n_final_false)<br>\n", "# b = n_final_false/s<br>\n", "# print(b)<br>\n", "# n_final = int(b)*s<br>\n", "# print(int(b)*s)<br>\n", "n_final = int((len(abdominal_signal)-1 - l + 1) / s )*s    # formula for nfinal<br>\n", "print(n_final)<br>\n", " Fix scale range and wavelet type to be used\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scales = np.arange(1,33)    # Need to choose this\n", "wavelet = 'gaus1'   # Need to choose this"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Freqeuncies in Hertz corresponding to the Scale range chosen above are as shown below\n<br>\n", "sampling_period =  0.001   # in secs<br>\n", "f = pywt.scale2frequency('gaus1', scales)/sampling_period     # in hertz<br>\n", "f<br>\n", " Compute **global vmin and vmax**\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["global_start_time = time.time()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vmin = 1 \n", "vmax = 0 "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i in range (0, n_final, s):"]}, {"cell_type": "markdown", "metadata": {}, "source": ["i = 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  sub_series_temp = abdominal_signal[i:i+l]   # this variable has to be an numpy.ndarray object\n", "  coeff, freqs = pywt.cwt(data = sub_series_temp, scales = scales, wavelet= wavelet)\n", "  mod_coeff = np.abs(coeff)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  if np.amin(mod_coeff) < vmin:\n", "    vmin = np.amin(mod_coeff)\n", "  else:\n", "    pass \n", "  if np.amax(mod_coeff) > vmax: \n", "    vmax = np.amax(mod_coeff)\n", "  else:\n", "    pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Runtime for computing vmin and vmax: \")\n", "print(\"--- %s seconds ---\" % (time.time() - global_start_time))\n", "#####################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nThis is how the generated scalograms would look like. <br>\n", "Use these in your report, PPT etc.<br>\n", "# Illustrative Scalogram generated below is with **local vmin and vmax**<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["for i in range (0, n_final, s):<br>\n", "for i in range (0, 2000, s):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["i = 1050\n", "# path = str(i/50) + '_illustrative_scalogram_with_local_vmin_and_vmax_for_abdominal_signal_' + str(z) + '.png'\n", "# path = str(i/50) + '_illustrative_scalogram_with_local_vmin_and_vmax_for_direct_signal.png'\n", "# path = str(i/50) + '_illustrative_scalogram_for_class_fetal_ECG.png'\n", "path = str(i/50) + '_illustrative_scalogram_for_class_NON_fetal_ECG.png'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sub_series_temp = abdominal_signal[i:i+l]   # this variable has to be a numpy.ndarray object\n", "coeff, freqs = pywt.cwt(data = sub_series_temp, scales = scales, wavelet= wavelet)\n", "mod_coeff = np.abs(coeff)\n", "t = np.array(np.arange(i,i+l,1) )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,5))\n", "plt.imshow(mod_coeff,\n", "          origin = 'lower',\n", "          #  cmap = 'hsv',\n", "           cmap = plt.cm.seismic,\n", "          # cmap = 'hot', \n", "          #  cmap = 'cool', \n", "            # cmap = 'coolwarm',\n", "          #  cmap = 'gray',\n", "          # cmap = 'magma',\n", "          # cmap = 'copper',\n", "          aspect = 'auto', \n", "          interpolation = 'bilinear',    # ????? some thought is needed\n", "          # vmin = vmin, \n", "          # vmax = vmax\n", "           vmin = mod_coeff.min(), \n", "           vmax = mod_coeff.max()\n", "           )\n", "plt.colorbar()\n", "plt.xlabel('Time (ms)')\n", "plt.ylabel('Scales')\n", "plt.title('Scalogram')\n", "plt.savefig(path)\n", "plt.show()\n", "plt.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nDiscuss with Nithin<br>\n", "What is this shit at around 300 ms (it should appear again, I am saying this by comparing the time domain graph of abdominal signal 1 with the scalogram plotted above this cell<br>\n", "# Illustrative Scalogram generated below is with **global vmin and vmax**<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["i = 1150"]}, {"cell_type": "markdown", "metadata": {}, "source": ["path = str(i/50) + '_illustrative_scalogram_with_global_vmin_and_vmax_for_abdominal_signal_' + str(z) + '.png'<br>\n", "path = str(i/50) + '_illustrative_scalogram_with_global_vmin_and_vmax_for_direct_signal.png'<br>\n", "path = str(i/50) + '_illustrative_scalogram_for_class_fetal_ECG_with_global_vmin_vmax.png'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["path = str(i/50) + '_illustrative_scalogram_for_class_NON_fetal_ECG_with_global_vmin_vmax.png'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sub_series_temp = abdominal_signal[i:i+l]   # this variable has to be a numpy.ndarray object\n", "coeff, freqs = pywt.cwt(data = sub_series_temp, scales = scales, wavelet= wavelet)\n", "mod_coeff = np.abs(coeff)\n", "t = np.array(np.arange(i,i+l,1) )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,5))\n", "plt.imshow(mod_coeff,\n", "          origin = 'lower',\n", "          #  cmap = 'hsv',\n", "           cmap = plt.cm.seismic,\n", "          # cmap = 'hot', \n", "          #  cmap = 'cool', \n", "            # cmap = 'coolwarm',\n", "          #  cmap = 'gray',\n", "          # cmap = 'magma',\n", "          # cmap = 'copper',\n", "          aspect = 'auto', \n", "          interpolation = 'bilinear',    # ????? some thought is needed\n", "          vmin = vmin, \n", "          vmax = vmax\n", "          #  vmin = mod_coeff.min(), \n", "          #  vmax = mod_coeff.max()\n", "           )\n", "plt.colorbar()\n", "plt.xlabel('Time (ms)')\n", "plt.ylabel('Scales')\n", "plt.title('Scalogram')\n", "plt.savefig(path)\n", "plt.show()\n", "plt.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Generate and save the Scalograms (using the **global vmin and vmax**)\n<br>\n", "global_start_time = time.time()<br>\n", "for i in range (0, n_final, s):<br>\n", "# i = 0<br>\n", "  # path = 'abdominal_signal_' + str(z) + '_scalograms_with_l_as_512/' + str(i/50) + '_scalogram.png'<br>\n", "  path = 'direct_signal_scalograms_with_l_as_512/' + str(i/50) + '_scalogram.png'<br>\n", "  sub_series_temp = abdominal_signal[i:i+l]   # this variable has to be a numpy.ndarray object<br>\n", "  coeff, freqs = pywt.cwt(data = sub_series_temp, scales = scales, wavelet= wavelet)<br>\n", "  mod_coeff = np.abs(coeff)<br>\n", "  # plt.figure(figsize=(8,5))<br>\n", "  plt.imsave(path, <br>\n", "             mod_coeff,<br>\n", "             origin = 'lower',<br>\n", "            #  cmap = 'hsv',<br>\n", "             cmap = plt.cm.seismic,             <br>\n", "            #  aspect = 'auto', <br>\n", "            #  interpolation = 'bilinear',    # ????? some thought is needed<br>\n", "             vmin = vmin, <br>\n", "             vmax = vmax)<br>\n", "  print(i)<br>\n", "print('Runtime for generating all the scalograms: ')<br>\n", "print(print(\"--- %s seconds ---\" % (time.time() - global_start_time)))<br>\n", "print('Number of scalograms generated = ' + str(i/50 +1))<br>\n", "print(n_final/50)<br>\n", " Labelleling the Scalograms generated (**as 'F' or 'N'**). Note that the labels will be exactly same for all five signal (four abdominal and one direct)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["true_labels_ = []   # contains the true annotations of fetal ECG signal (in seconds)\n", "for i in range (0, len(true_labels)):\n", "  true_labels_.append(true_labels[i][0])\n", "true_labels_df = pd.DataFrame(true_labels_)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lst_temp_1 = []\n", "lst_temp_2 = []\n", "for i in range (0, n_final, s):\n", "  lst_temp_1.append(i/50)\n", "  lst_temp_2.append((i+l)/1000)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lst_temp_3 = []\n", "k = 0\n", "for i in range (0, len(true_labels_)):\n", "  temp_label = true_labels_[i]\n", "  for j in range (k, len(lst_temp_2)):\n", "    temp_last_time_point = lst_temp_2[j]\n", "    if temp_label < temp_last_time_point:\n", "      lst_temp_3.append(1)\n", "      k = j+1\n", "      break \n", "    else: #temp_label > temp_last_time_point: \n", "      lst_temp_3.append(0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scalogram_labels_df = pd.DataFrame({'Scalogram ID': lst_temp_1,\n", "                                    'Last Time Instant in Scalogram (in sec)': lst_temp_2, \n", "                                    'labels_created': lst_temp_3\n", "})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["true_labels_df.to_csv('true_labels_df.csv')\n", "scalogram_labels_df.to_csv('scalogram_labels_df.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nCreat training and testing datasets<br>\n", "# Approach 1: Create 12 channels using the 3 RGB channels of each of the 4 images<br>\n", "Approach 2: Create 4 cahnnels using mod_coeff directly for 4 CWTs<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Approach 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Read the images as arrays<br>\n", "from skimage.io import imread"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from cv2 import imread"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["global_start_time = time.time()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["all_images = []\n", "for i in range (0, n_final, s):\n", "# i = 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  dic_img = {}\n", "  for z in range(0,5):\n", "  # z = 1\n", "    path_here = 'abdominal_signal_' + str(z) + '_scalograms_with_l_as_512/' + str(i/50) + '_scalogram.png'\n", "    dic_img[z] = imread(path_here)\n", "  img_with_12_channels = np.concatenate((dic_img[1], dic_img[2], dic_img[3], dic_img[4]), axis = 2)\n", "  all_images.append(img_with_12_channels)\n", "  print('Done: ' +str(i/50) + ' Remaining: ' + str((n_final-i)/50))\n", "x = np.array(all_images)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Runtime for obtaining x: ')\n", "print(print(\"--- %s seconds ---\" % (time.time() - global_start_time)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.save('x_array', x)    # save the array x as .npy file in drive"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["start_time = time.time()\n", "x = np.load('x_array.npy')    # load the array x using x_array.npy file in drive\n", "print('Runtime for loading x: ')\n", "print(\"--- %s seconds ---\" % (time.time() - start_time))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(x.shape)\n", "print(type(x))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Transfer Learning using VGG16 architechture (for feature vector extraction from Scalograms of a particular Abdominal Signal)\n<br>\n", "from keras.applications.vgg16 import VGG16<br>\n", "from keras.preprocessing import image<br>\n", "from keras.applications.vgg16 import preprocess_input<br>\n", "# load VGG16 model weights (pre-trained)<br>\n", "model = VGG16(weights='imagenet',<br>\n", "              include_top=False, <br>\n", "              pooling = 'avg')<br>\n", "z = 2     # abdominal signal under consideration<br>\n", "# ##############################################################################<br>\n", "# This cell takes around 35 mins to execute using NVIDIA TESLA K80 GPU (Around 4 GB RAM, 50 GB Memory is needed)<br>\n", "# ##############################################################################<br>\n", "# Extracting features (for all scalograms) using the VGG16 pre-trained model<br>\n", "global_start_time_1 = time.time()<br>\n", "df_for_feature_using_VGG16 = pd.DataFrame(columns = None) <br>\n", "for j in range (0, n_final, s):<br>\n", "# j = 0<br>\n", "  image_path = 'abdominal_signal_' + str(z) + '_scalograms_with_l_as_512/' + str(j/50) + '_scalogram.png'<br>\n", "  img = image.load_img(image_path)<br>\n", "  # print(file_path_here)<br>\n", "  x = image.img_to_array(img)<br>\n", "  # print(x.shape)<br>\n", "  x = np.expand_dims(x, axis = 0)<br>\n", "  # print(x.shape)<br>\n", "  x = preprocess_input(x)<br>\n", "  features = model.predict(x)[0]<br>\n", "  features_arr = np.char.mod('%f', features) <br>\n", "  df_for_feature_using_VGG16['FeatureVector ' + str(j/50)] = features_arr  <br>\n", "  print(str(j/50))<br>\n", "print(print(\"--- %s seconds ---\" % (time.time() - global_start_time_1)))<br>\n", "df_for_feature_using_VGG16.to_csv('df_for_feature_using_VGG16_for_abdominal_signal_' + str(z) + '.csv') <br>\n", "print(df_for_feature_using_VGG16.shape )<br>\n", "print(df_for_feature_using_VGG16.head())<br>\n", " Load Feature Vectors generated and stored earlier\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["z = 4"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##########################################################################<br>\n", "Run this block instead as \"df_for_feature_using_VGG16_for_abdominal_signal_z.csv\" is already created once and stroed in drive <br>\n", "##########################################################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_for_feature_using_VGG16 = pd.read_csv('df_for_feature_using_VGG16_for_abdominal_signal_' + str(z) + '.csv')\n", "del df_for_feature_using_VGG16['Unnamed: 0']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(df_for_feature_using_VGG16.head())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# MLP after feature extraction\n<br>\n", "from keras import utils<br>\n", "X_data = [df_for_feature_using_VGG16['FeatureVector ' + str(i) + '.0'].values for i in range(0,5989)]<br>\n", "Y_data = scalogram_labels_df['labels_created'].values.tolist()<br>\n", "# exclude the first observation from the dataset<br>\n", "X_data = X_data[1:]<br>\n", "Y_data = Y_data[1:]<br>\n", "x_train = np.array(X_data[:4192])<br>\n", "y_train = np.array(Y_data[:4192])<br>\n", "x_test = np.array(X_data[4192:])<br>\n", "y_test = np.array(Y_data[4192:])<br>\n", "NUM_CLASSES = 2<br>\n", "train_labels = utils.to_categorical(y_train, NUM_CLASSES)<br>\n", "test_labels  = utils.to_categorical(y_test, NUM_CLASSES)<br>\n", " few things to see if you loose track\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["5988*0.7"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_labels[:10]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_labels.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Back on MLP\n<br>\n", "# creating a MLP<br>\n", "import keras<br>\n", "from keras.models import Sequential<br>\n", "from keras.layers import Dense, Activation<br>\n", "# from keras.callbacks import history <br>\n", "model2 = Sequential()<br>\n", "model2.add(Dense(1024, input_dim=512, activation='relu',kernel_initializer='uniform'))<br>\n", "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)<br>\n", "model2.add(Dense(1024,input_dim=1024,activation='sigmoid'))<br>\n", "keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)<br>\n", "model2.add(Dense(512,input_dim=1024,activation='sigmoid'))<br>\n", "keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)<br>\n", "model2.add(Dense(units=2))<br>\n", "model2.add(Activation('softmax'))<br>\n", "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])<br>\n", "model2.summary()<br>\n", "# Initializing earlystopping callback<br>\n", "from keras import callbacks<br>\n", "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", <br>\n", "                                        mode =\"min\", patience = 5, <br>\n", "                                        restore_best_weights = True)<br>\n", "# fitting/training the model <br>\n", "history = model2.fit(x_train,<br>\n", "           train_labels,<br>\n", "           epochs=200,<br>\n", "           batch_size=16,<br>\n", "           verbose = 1,<br>\n", "           validation_split = 0.2, <br>\n", "           shuffle = False,<br>\n", "           callbacks =[earlystopping])<br>\n", "plt.plot(history.history['accuracy'])<br>\n", "plt.plot(history.history['val_accuracy'])<br>\n", "plt.title('Model Accuracy')<br>\n", "plt.ylabel('Accuracy')<br>\n", "plt.xlabel('Epoch')<br>\n", "plt.legend(['train', 'val'], loc='upper left')<br>\n", "plt.savefig('Model_Accuracy_Plot_for_abdominal_signal_' + str(z) + '.png')<br>\n", "plt.show()<br>\n", "plt.close()<br>\n", "plt.plot(history.history['loss'])<br>\n", "plt.plot(history.history['val_loss'])<br>\n", "plt.title('Model Loss Function Plot')<br>\n", "plt.ylabel('Loss')<br>\n", "plt.xlabel('Epoch')<br>\n", "plt.legend(['train', 'val'], loc='upper left')<br>\n", "plt.savefig('Model_Loss_Function_Plot_for_abdominal_signal_' + str(z) + '.png')<br>\n", "plt.show()<br>\n", "plt.close()<br>\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$<br>\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$<br>\n", "Perhaps you'll have hard time following the code from <br>\n", "here onwards. But I'm sure you'll manage it :p<br>\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$<br>\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$<br>\n", " Results of MLP\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["predictions = model2.predict_classes(x_test)\n", "predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(sum(predictions))\n", "print(predictions.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(sum(test_labels[:,1]))\n", "print(test_labels[:,1].shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_results = pd.DataFrame({'salogram_id': np.arange(4193,5989),\n", "                            'true_scalogram_labels': test_labels[:,1],\n", "                            'predicted_scalogram_labels': predictions\n", "})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_results.to_csv('prediction_results_for_abdominal_signal_' + str(z) + '.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\n", "cnf_matrix = confusion_matrix(test_labels[:,1], \n", "                              predictions)\n", "# print(cnf_matrix)\n", "np.save('cnf_matrix_for_abdominal_signal_' + str(z) + '.npy', cnf_matrix)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cnf_matrix"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Ensemble Model \n<br>\n", "confusion_matrices = []<br>\n", "for i in range(1,5):<br>\n", "  cnf_matrix_temp = np.load('cnf_matrix_for_abdominal_signal_' + str(i) + '.npy')<br>\n", "  # print('Confusion Matrix for Abdominal Signal ' + str(i) + ' :')<br>\n", "  # print(cnf_matrix_temp)<br>\n", "  # confusion_matrices.append(cnf_matrix_temp)<br>\n", "  labels = ['True Neg','False Pos','False Neg','True Pos']<br>\n", "  categories = [0, 1]<br>\n", "  make_confusion_matrix(cnf_matrix_temp, <br>\n", "                        group_names=labels,<br>\n", "                        categories=categories, <br>\n", "                        cmap='binary')<br>\n", "import matplotlib.pyplot as plt<br>\n", "for matrix in confusion_matrices:<br>\n", "    fig = plt.figure()<br>\n", "    # plt.matshow(cm)<br>\n", "    plt.title('Problem 1: Confusion Matrix Digit Recognition')<br>\n", "    # plt.colorbar()<br>\n", "    plt.ylabel('True Label')<br>\n", "    plt.xlabel('Predicated Label')<br>\n", "    plt.savefig('confusion_matrix'+str(learning_values.pop())+'.jpg')<br>\n", "df_for_truth_labels_temp = pd.read_csv('prediction_results_for_abdominal_signal_1.csv')<br>\n", "truth  = df_for_truth_labels_temp['true_scalogram_labels']<br>\n", "prediction_dic = {}<br>\n", "for i in range(1,5):<br>\n", "  df_for_prediction_temp = pd.read_csv('prediction_results_for_abdominal_signal_' + str(i) + '.csv')<br>\n", "  prediction_dic[i] = df_for_prediction_temp['predicted_scalogram_labels']<br>\n", "df_for_predictions_all_signals = pd.DataFrame({'Predictions for Abdominal Signal 1': prediction_dic[1],<br>\n", "                                               'Predictions for Abdominal Signal 2': prediction_dic[2],<br>\n", "                                               'Predictions for Abdominal Signal 3': prediction_dic[3],<br>\n", "                                               'Predictions for Abdominal Signal 4': prediction_dic[4]    <br>\n", "})<br>\n", "df_for_predictions_all_signals.head()<br>\n", "# Ensemble model 1<br>\n", "# Criteria (Criteria 1): Predictions for at least 3/4 signals should be 1 for 'ensemble_prediction' to be 1<br>\n", "predictions_ensemble_with_criteria_1 = []<br>\n", "for i in range (0, len(df_for_predictions_all_signals)):<br>\n", "  sum_val = df_for_predictions_all_signals['Predictions for Abdominal Signal 1'][i] + df_for_predictions_all_signals['Predictions for Abdominal Signal 2'][i] + df_for_predictions_all_signals['Predictions for Abdominal Signal 3'][i] + df_for_predictions_all_signals['Predictions for Abdominal Signal 4'][i]<br>\n", "  if sum_val > 2 : <br>\n", "    predictions_ensemble_with_criteria_1.append(1)<br>\n", "  else: <br>\n", "    predictions_ensemble_with_criteria_1.append(0)<br>\n", "df_for_predictions_all_signals['Predictions for Ensemble with Criteria 1'] = predictions_ensemble_with_criteria_1<br>\n", "# Ensemble model 2<br>\n", "# Criteria (Criteria 2): Predictions for at least 2/4 signals should be 1 for 'ensemble_prediction' to be 1<br>\n", "predictions_ensemble_with_criteria_2 = []<br>\n", "for i in range (0, len(df_for_predictions_all_signals)):<br>\n", "  sum_val = df_for_predictions_all_signals['Predictions for Abdominal Signal 1'][i] + df_for_predictions_all_signals['Predictions for Abdominal Signal 2'][i] + df_for_predictions_all_signals['Predictions for Abdominal Signal 3'][i] + df_for_predictions_all_signals['Predictions for Abdominal Signal 4'][i]<br>\n", "  if sum_val > 1 : <br>\n", "    predictions_ensemble_with_criteria_2.append(1)<br>\n", "  else: <br>\n", "    predictions_ensemble_with_criteria_2.append(0)<br>\n", "df_for_predictions_all_signals['Predictions for Ensemble with Criteria 2'] = predictions_ensemble_with_criteria_2<br>\n", "# Let's add truth too in the same dataframe for consistency<br>\n", "df_for_predictions_all_signals['True Labels'] = truth<br>\n", "# # Let's add truth too in the same dataframe for consistency<br>\n", "# df_for_predictions_all_signals['True Labels'] = truth<br>\n", "from sklearn.metrics import confusion_matrix<br>\n", "cnf_matrix_for_ensemble_1 = confusion_matrix(df_for_predictions_all_signals['True Labels'].values, <br>\n", "                              df_for_predictions_all_signals['Predictions for Ensemble with Criteria 1'])<br>\n", "np.save('cnf_matrix_for_ensemble_1.npy', cnf_matrix_for_ensemble_1)<br>\n", "labels = ['True Neg','False Pos','False Neg','True Pos']<br>\n", "categories = [0, 1]<br>\n", "make_confusion_matrix(cnf_matrix_for_ensemble_1, <br>\n", "                      group_names=labels,<br>\n", "                      categories=categories, <br>\n", "                      cmap='binary')<br>\n", "cnf_matrix_for_ensemble_2 = confusion_matrix(df_for_predictions_all_signals['True Labels'].values, <br>\n", "                              df_for_predictions_all_signals['Predictions for Ensemble with Criteria 2'])<br>\n", "np.save('cnf_matrix_for_ensemble_2.npy', cnf_matrix_for_ensemble_2)<br>\n", "labels = ['True Neg','False Pos','False Neg','True Pos']<br>\n", "categories = [0, 1]<br>\n", "make_confusion_matrix(cnf_matrix_for_ensemble_2, <br>\n", "                      group_names=labels,<br>\n", "                      categories=categories, <br>\n", "                      cmap='binary')<br>\n", "# print('Confusion Matrix for Ensemble Model 1: ')<br>\n", "# print(cnf_matrix_for_ensemble_1)<br>\n", "# print('Confusion Matrix for Ensemble Model 2: ')<br>\n", "# print(cnf_matrix_for_ensemble_2)<br>\n", "df_for_predictions_all_signals.to_csv('df_for_predictions_all_signals_and_ensembles_both.csv')<br>\n", "import pandas as pd<br>\n", "import numpy as np<br>\n", "df_for_predictions_all_signals = pd.read_csv('df_for_predictions_all_signals_and_ensembles_both.csv')<br>\n", "del df_for_predictions_all_signals['Unnamed: 0']<br>\n", "# df_for_predictions_all_signals.head()<br>\n", "df_for_predictions_all_signals[:20]<br>\n", "from sklearn.metrics import confusion_matrix<br>\n", "cnf_matrix_ = confusion_matrix(df_for_predictions_all_signals['True Labels'].values,<br>\n", "                                  df_for_predictions_all_signals['Predictions for Abdominal Signal 4'], labels = [0, 1])<br>\n", "tn, fp, fn, tp = confusion_matrix(df_for_predictions_all_signals['True Labels'].values,<br>\n", "                                  df_for_predictions_all_signals['Predictions for Abdominal Signal 4']).ravel()<br>\n", "print('tn is ' + str(tn))<br>\n", "print('fp is ' + str(fp))<br>\n", "print('fn is ' + str(fn))<br>\n", "print('tp is ' + str(tp))<br>\n", "print(cnf_matrix_)<br>\n", "import seaborn as sns<br>\n", "# # sns.heatmap(cnf_matrix_, annot=True)<br>\n", "# group_names = ['True Neg','False Pos','False Neg','True Pos']<br>\n", "# group_counts = [\"{0:0.0f}\".format(value) for value in<br>\n", "#                 cnf_matrix_.flatten()]<br>\n", "# group_percentages = [\"{0:.2%}\".format(value) for value in<br>\n", "#                      cnf_matrix_.flatten()/np.sum(cnf_matrix_)]<br>\n", "# labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in<br>\n", "#           zip(group_names,group_counts,group_percentages)]<br>\n", "# labels = np.asarray(labels).reshape(2,2)<br>\n", "# sns.heatmap(cnf_matrix_, annot=labels, fmt='', cmap='Blues')<br>\n", "labels = ['True Neg','False Pos','False Neg','True Pos']<br>\n", "categories = [0, 1]<br>\n", "make_confusion_matrix(cnf_matrix_, <br>\n", "                      group_names=labels,<br>\n", "                      categories=categories, <br>\n", "                      cmap='binary')<br>\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$<br>\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$<br>\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$<br>\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$<br>\n", "import numpy as np<br>\n", "import matplotlib.pyplot as plt<br>\n", "import seaborn as sns<br>\n", "def make_confusion_matrix(cf,<br>\n", "                          group_names=None,<br>\n", "                          categories='auto',<br>\n", "                          count=True,<br>\n", "                          percent=True,<br>\n", "                          cbar=True,<br>\n", "                          xyticks=True,<br>\n", "                          xyplotlabels=True,<br>\n", "                          sum_stats=True,<br>\n", "                          figsize=None,<br>\n", "                          cmap='Blues',<br>\n", "                          title=None):<br>\n", "  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n", "    Arguments\n", "    ---------\n", "    cf:            confusion matrix to be passed in\n", "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n", "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n", "    count:         If True, show the raw number in the confusion matrix. Default is True.\n", "    normalize:     If True, show the proportions for each category. Default is True.\n", "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n", "                   Default is True.\n", "    xyticks:       If True, show x and y ticks. Default is True.\n", "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n", "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n", "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n", "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n", "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n", "                   \n", "    title:         Title for the heatmap. Default is None.\n", "    '''"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n", "    blanks = ['' for i in range(cf.size)]\n", "    if group_names and len(group_names)==cf.size:\n", "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n", "    else:\n", "        group_labels = blanks\n", "    if count:\n", "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n", "    else:\n", "        group_counts = blanks\n", "    if percent:\n", "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n", "    else:\n", "        group_percentages = blanks\n", "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n", "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n", "    if sum_stats:\n", "        #Accuracy is sum of diagonal divided by total observations\n", "        accuracy  = np.trace(cf) / float(np.sum(cf))\n\n", "        #if it is a binary confusion matrix, show some more stats\n", "        if len(cf)==2:\n", "            #Metrics for Binary Confusion Matrices\n", "            precision = cf[1,1] / sum(cf[:,1])\n", "            recall    = cf[1,1] / sum(cf[1,:])\n", "            f1_score  = 2*precision*recall / (precision + recall)\n", "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n", "                accuracy,precision,recall,f1_score)\n", "        else:\n", "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n", "    else:\n", "        stats_text = \"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n", "    if figsize==None:\n", "        #Get default figure size if not set\n", "        figsize = plt.rcParams.get('figure.figsize')\n", "    if xyticks==False:\n", "        #Do not show categories if xyticks is False\n", "        categories=False"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # MAKE THE HEATMAP VISUALIZATION\n", "    plt.figure(figsize=figsize)\n", "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n", "    if xyplotlabels:\n", "        plt.ylabel('True label')\n", "        plt.xlabel('Predicted label' + stats_text)\n", "    else:\n", "        plt.xlabel(stats_text)\n", "    \n", "    if title:\n", "        plt.title(title)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["precision = tp/(tp+fp)\n", "print('precision is ' + str(precision))\n", "recall = tp/(tp+fn)\n", "print('recall is ' + str(recall))\n", "specificity = tn/(tn+fp)\n", "print('specificity is ' + str(specificity))\n", "print('sensitiviy is ' + str(recall))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n", "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "# CNN after Transfer Learning (INCOMPLETE WORK)<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from keras.layers import Dense, Flatten\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.models import Sequential\n", "from keras.callbacks import History \n", "history = History()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = 1\n", "y = 512\n", "z = 1\n", "input_shape = (x, y, z)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_classes = 2\n", "batch_size = 128\n", "epochs = 20"]}, {"cell_type": "markdown", "metadata": {}, "source": ["x_train = x_train.astype('float32')<br>\n", "x_test = x_test.astype('float32')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model3 = Sequential()\n", "model3.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n", "                 activation='relu',\n", "                 input_shape=input_shape))\n", "model3.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "model3.add(Conv2D(64, (3, 3), activation='relu'))\n", "model3.add(MaxPooling2D(pool_size=(2, 2)))\n", "model3.add(Flatten())\n", "model3.add(Dense(1000, activation='relu'))\n", "model3.add(Dense(num_classes, activation='softmax'))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model3.compile(loss=keras.losses.categorical_crossentropy,\n", "              optimizer=keras.optimizers.Adam(),\n", "              metrics=['accuracy'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["fit the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model3.fit(x_train, y_train,\n", "          batch_size=batch_size,\n", "          epochs=epochs,\n", "          verbose=1,\n", "          validation_data=(x_test, y_test),\n", "          callbacks=[history])"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}